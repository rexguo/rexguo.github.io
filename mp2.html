<!--
To serve videos from S3 to a static website on a different domain:

1. CORS (JSON) in S3 bucket settings:
[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "GET",
            "HEAD"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": [
            "Cross-Origin-Embedder-Policy",
            "Cross-Origin-Resource-Policy",
            "Cross-Origin-Opener-Policy",
            "Accept-Ranges",
            "Content-Range",
            "Content-Length"
        ],
        "MaxAgeSeconds": 3000
    }
]

2. Make file publicly accessible in S3 under object permissions

3. Use this HTML headers:
  <meta http-equiv="Access-Control-Allow-Origin" content="*">
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">

4. Add crossorigin="anonymous" to the video tag:
  <video width="640" height="360" controls crossorigin="anonymous">

-->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="Cache-control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta http-equiv="Access-Control-Allow-Origin" content="*">
  <!-- <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp"> -->
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <meta http-equiv="Permissions-Policy" content="camera=(), microphone=(), geolocation=(), interest-cohort=()">
  <title>Face Detector</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined" rel="stylesheet">
  <script type="importmap">
    {
      "imports": {
        "@material/web/": "https://esm.run/@material/web/"
      }
    }
  </script>
  <script type="module">
    import '@material/web/all.js';
    import {styles as typescaleStyles} from '@material/web/typography/md-typescale-styles.js';

    document.adoptedStyleSheets.push(typescaleStyles.styleSheet);
  </script>
  
  <!-- <link href="./material-components-web.min.css" rel="stylesheet"> -->
  <!-- <script src="./material-components-web.min.js"></script> -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lottie-web/5.9.6/lottie.min.js"></script>
  <!-- <script src="./lottie-5.9.6.min.js"></script> -->
  <!-- <link rel="stylesheet" href="mp.css"> -->
  <style>
      #bdDiv {
          width: 200px;
          height: 200px;
          background-color: rgba(0,0,0,0);
          position: absolute;
          cursor: pointer;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          font-family: Arial, sans-serif;
      }

  video::-webkit-media-controls
  {
  display: flex !important;
  }

      /* Add new styles for the button */
      md-filled-button {
        --md-filled-button-container-shape: 8px;
      }
      
      md-filled-button md-icon {
        font-size: 20px;
        margin-right: 8px;
        display: flex;
        align-items: center;
      }

      md-filled-button span {
        line-height: 20px;
        display: flex;
        align-items: center;
      }

      .md-sys-typescale-body-medium {
        font-family: 'Roboto', sans-serif;
      }

  /* Add styles for the face size display */
  #faceSizeDisplay {
    /* position: absolute; */
    /* top: 10px; */
    /* left: 10px; */
    background-color: rgba(0, 0, 0, 0.5);
    color: white;
    padding: 5px;
    border-radius: 5px;
    font-family: 'Roboto', sans-serif;
  }

</style>
</head>
<body>
<script>
var bdSize = 200, userX = 0, userY = -100, userR = 0;
let S3 = "https://rexguo.s3.ap-southeast-1.amazonaws.com/face/";
</script>


    <!-- Create a container for all controls -->
    <div style="display: flex; flex-direction: column; gap: 20px; margin-bottom: 20px;">
      <!-- Video source selectors -->
      <div style="display: flex; align-items: center; gap: 20px;">
        <label class="md-sys-typescale-body-medium" style="min-width: 80px;">Select source:</label>

        <div style="display: flex; align-items: center; gap: 10px;">
          <md-filled-button id="webcamStartButton" style="display: flex; align-items: center;">
            <span>Start Webcam</span>
          </md-filled-button>
          <label class="md-sys-typescale-body-medium" style="min-width: 80px;">Built-in Video:</label>
          <md-outlined-select id="builtinVideoSelector" style="min-width: 200px;">
            <md-select-option selected value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/matrix-no-spoon.mov">Matrix No Spoon</md-select-option>
            <md-select-option value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/bbc-news-anchor-trimmed.mp4">BBC News Anchor</md-select-option>
            <md-select-option value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/Daniel Craig EXPOSES Queen Elizabeth for Making Fun of Him!.mp4">Daniel Craig</md-select-option>
            <md-select-option value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/Avengers (2012).mp4">Avengers</md-select-option>
            <md-select-option value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/Obama+to+heckler+You're+screwing+up+my+speech.mp4">Obama</md-select-option>
            <md-select-option value="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/Obama's+Quick+Response+To+Sarcastic+Republican+Clappers.mp4">Obama2</md-select-option>
          </md-out-select>
        </div>
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 80px;">Upload video:</label>
          <md-filled-button id="videoButton" style="display: flex; align-items: center;">
            <!-- <md-icon class="material-symbols-outlined">upload_file</md-icon> -->
            <span>Choose File</span>
          </md-filled-button>
          <input type="file" id="videoFileInput" accept="video/*" style="display: none;">
        </div>
        <md-filled-button id="startButton" style="display: flex; align-items: center; --md-filled-button-container-color: #FFA500;">
          <span>START</span>
        </md-filled-button>
      </div>

      <!-- Animation controllers -->
      <div style="display: flex; align-items: center; gap: 20px;">
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 80px;">Animation:</label>
          <md-outlined-select id="jsonSelector" style="min-width: 150px; width: 150px;">
            <md-select-option selected value="crown-good-1736777171661.json">Crown 1</md-select-option>
            <md-select-option value="crown-1736776829639.json">Crown 2</md-select-option>
            <md-select-option value="crown-1736776989009.json">Crown 3</md-select-option>
            <md-select-option value="crown-1736777037913.json">Crown 4</md-select-option>
            <md-select-option value="crown-1736777065980.json">Crown 5</md-select-option>
            <md-select-option value="bird1.json">Bird 1</md-select-option>
            <md-select-option value="bird-1736777404559.json">Bird 2</md-select-option>
            <md-select-option value="bird-1736777461770.json">Bird 3</md-select-option>
            <md-select-option value="bird-1736777509527.json">Bird 4</md-select-option>
            <md-select-option value="drink.json">Drink</md-select-option>
          </md-outlined-select>
        </div>
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 30px;">Size:</label>
          <md-slider id="sizeSlider" style="min-width: 140px;" min="50" max="400" value="200" labeled ticks></md-slider>
        </div>
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 20px;">X:</label>
          <md-slider id="xSlider" style="min-width: 140px;" min="-200" max="200" value="0" labeled ticks></md-slider>
        </div>
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 20px;">Y:</label>
          <md-slider id="ySlider" style="min-width: 140px;" min="-300" max="100" value="-100" labeled ticks></md-slider>
        </div>
        <div style="display: flex; align-items: center; gap: 10px;">
          <label class="md-sys-typescale-body-medium" style="min-width: 20px;">R:</label>
          <md-slider id="rotSlider" style="min-width: 140px;" min="-90" max="90" value="0" labeled ticks></md-slider>
        </div>
      </div>
    </div>
  <section id="demos" class="invisible">
    <div id="liveView" class="videoView">

      <!-- Add a container div with relative positioning -->
      <div class="video-container" style="position: relative; width: fit-content;">
        <video id="video" style="display: block;" width="1000" height="562" loop controls crossorigin="anonymous">
          <source src="https://rexguo.s3.ap-southeast-1.amazonaws.com/face/matrix-no-spoon.mov" type="video/mp4">
        </video>
        <!-- Wrapper for canvas and BD with clipping -->
        <div class="overlay-container" style="position: absolute; left: 0; top: 0; width: 100%; height: 100%; overflow: hidden;">
          <canvas class="output_canvas" id="output_canvas" style="position: absolute; left: 0; top: 0;"></canvas>
          <div id="bdDiv" class="bd"></div>
        </div>
      </div>
    </div>
  <div id="faceSizeDisplay" width="1000">Face size: N/A</div>
<script>

// Move this to top of script section
// Add click handler to button that triggers file input
document.getElementById('videoButton').addEventListener('click', () => {
  document.getElementById('videoFileInput').click();
});

// Handle video file input change
document.getElementById('videoFileInput').addEventListener('change', function(event) {
  const file = event.target.files[0];
  if (file) {
    const video = document.getElementById('video');
    const source = video.querySelector('source');
    const url = URL.createObjectURL(file);
    source.src = url;
    video.load();
  }
});

// Handle built-in video selection change
document.getElementById('builtinVideoSelector').addEventListener('change', function(event) {
  const video = document.getElementById('video');
  const source = video.querySelector('source');
  source.src = event.target.value;
  video.load();
});

let container = document.querySelector('.bd');
let animItem;

function loadAnimation(path) {
  if (animItem) {
    animItem.destroy();
  }
  animItem = bodymovin.loadAnimation({
    wrapper: container,
    animType: 'canvas',
    loop: true,
    path: path
  });

  console.log(animItem);

  // Add onComplete event listener to pause animation if video is paused
  animItem.addEventListener('DOMLoaded', () => {
    if (video.paused) {
      animItem.pause();
    }
  });
}

// Add video event listeners to control Lottie animation
const video = document.getElementById('video');
video.addEventListener('play', () => {
  if (animItem) {
    animItem.play();
  }
});

video.addEventListener('pause', () => {
  if (animItem) {
    animItem.pause();
  }
});

// Load default BD
loadAnimation(S3+'crown-good-1736777171661.json');

// Update the jsonSelector change handler to use the CORS proxy
document.getElementById('jsonSelector').addEventListener('change', function(e) {
  loadAnimation(S3 + e.target.value);
  // Set initial play state based on video
  if (animItem && video.paused) {
    animItem.pause();
  }
});

const div = document.getElementById('bdDiv');
//div.style.display = 'none'; // start invisible until face is detected

let isDragging = false;
let currentX = 0;
let currentY = 0;
let initialX = 0;
let initialY = 0;
let currentRotation = 0;

document.getElementById('xSlider').addEventListener('input', (e) => {
  userX = Number(e.target.value);
});

document.getElementById('ySlider').addEventListener('input', (e) => {
  userY = Number(e.target.value);
});

document.getElementById('rotSlider').addEventListener('input', (e) => {
  userR = Number(e.target.value);
});

// Add Material 3 slider event listener
document.getElementById('sizeSlider').addEventListener('input', (e) => {
  bdSize = e.target.value;
  const bdDiv = document.getElementById('bdDiv');
  bdDiv.style.width = bdSize + 'px';
  bdDiv.style.height = bdSize + 'px';
});

// Enable drag functionality
div.addEventListener('mousedown', startDragging);
document.addEventListener('mousemove', drag);
document.addEventListener('mouseup', stopDragging);

function startDragging(e)
{
  console.log("startDragging");
  initialX = e.clientX - currentX;
  initialY = e.clientY - currentY;
  isDragging = true;
}

function drag(e)
{
  if(isDragging)
  {
    switch(e.button)
    {
    case 0:
      e.preventDefault();
      currentX = e.clientX - initialX;
      currentY = e.clientY - initialY;
      updateBDPosition(currentX, currentY);
      break;

    case 1:

      break;

    case 2: // not working because context menu pops up

      e.preventDefault();
      const angle = Math.atan2(e.clientX - currentX, 
                               e.clientY - currentY);
      const degrees = angle * (180 / Math.PI);

      div.style.transform = `rotate(${degrees}deg)`;
      break;
    }
  }
}

function stopDragging()
{
  isDragging = false;
}

function updateBDPosition(x, y, rz, scale = 1.0)
{
  //console.log("setBD x,y,rz: ", x.toFixed(2), y.toFixed(2), rz.toFixed(2));

  const element = document.querySelector("#bdDiv");
  const styles  = window.getComputedStyle(element);

  const half = (scale * bdSize) / 2;
  //const half = bdSize / 2;

  // Convert rz from degrees to radians
  const radians = rz * (Math.PI / 180);

  const ux = userX * scale;
  const uy = userY * scale;
  
  // Transform userX and userY by the given angle rz. This transformation
  // will have the origin based on (x, y). Eg from between 2 eyes.
  const tx = ux * Math.cos(radians) - uy * Math.sin(radians);
  const ty = ux * Math.sin(radians) + uy * Math.cos(radians);

  const canvas = document.getElementById("output_canvas");// as HTMLCanvasElement;
  //const canvas = canvasCtx.canvas;
  const cw = canvas.width, ch = canvas.height;

  // const ve = document.getElementById('video');
  // const vw = ve.clientWidth;
  // const vh = ve.clientHeight;

  // const r = vw / cw;

  // Coordinate space of the div is what is defined in the HTML video element:
  // <video id="video" width="1000" height="562" ...>
  //div.style.left = '800px';//(1*(r*x-half + tx))+'px';
  //div.style.top  = '0px';//(1*(r*y-half + ty))+'px';
  div.style.left = (x-half + tx)+'px';
  div.style.top  = (y-half + ty)+'px';

  //let r = rz + userR;

  div.style.width  = (scale * bdSize) + 'px';
  div.style.height = (scale * bdSize) + 'px';

  div.style.transform = `rotate(${rz + userR}deg)`;
}

function showBD(b) {
  // if (animItem) {
  //   animItem.play();
  // }

  // const canvas = document.getElementById('output_canvas');
  // canvas.style.display = 'block';

  if(b)
  {
    console.log("BD ON");
    const bdDiv = document.getElementById('bdDiv');
    bdDiv.style.display = 'flex';
  }
  else
  {
    console.log("BD OFF");
    const bdDiv = document.getElementById('bdDiv');
    bdDiv.style.display = 'none';
  }
}
</script>

    <div class="blend-shapes">
      <ul class="blend-shapes-list" id="video-blend-shapes"></ul>
    </div>
  </section>
<script type="module">
// Copyright 2023 The MediaPipe Authors.

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//      http://www.apache.org/licenses/LICENSE-2.0

// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
//import vision from "./mp-tasks-vision@0.10.3.js";
//import vision from "https://rexguo.s3.ap-southeast-1.amazonaws.com/face/mp-tasks-vision@0.10.3.js";

const { GestureRecognizer,
        ObjectDetector, Detection, ObjectDetectionResult, 
        PoseLandmarker, ImageSegmenter, SegmentationMask, 
        FaceLandmarker, FilesetResolver, DrawingUtils } = vision;
const demosSection     = document.getElementById("demos");
const imageBlendShapes = document.getElementById("image-blend-shapes");
const videoBlendShapes = document.getElementById("video-blend-shapes");

let faceLandmarker, imageSegmenter, poseLandmarker, objectDetector, gestureRecognizer;
let runningMode = "VIDEO";//: "IMAGE" | "VIDEO" = "IMAGE";
let enableWebcamButton = HTMLButtonElement;
let webcamRunning = false;//: Boolean = false;
const videoWidth = 1000;//480;

// segmentation:
const resultWidthHeigth = 256;

//let imageSegmenter: ImageSegmenter;
let labels;//: Array<string>;

const legendColors = [
  [255, 197, 0, 255], // Vivid Yellow
  [128, 62, 117, 255], // Strong Purple
  [255, 104, 0, 255], // Vivid Orange
  [166, 189, 215, 255], // Very Light Blue
  [193, 0, 32, 255], // Vivid Red
  [206, 162, 98, 255], // Grayish Yellow
  [129, 112, 102, 255], // Medium Gray
  [0, 125, 52, 255], // Vivid Green
  [246, 118, 142, 255], // Strong Purplish Pink
  [0, 83, 138, 255], // Strong Blue
  [255, 112, 92, 255], // Strong Yellowish Pink
  [83, 55, 112, 255], // Strong Violet
  [255, 142, 0, 255], // Vivid Orange Yellow
  [179, 40, 81, 255], // Strong Purplish Red
  [244, 200, 0, 255], // Vivid Greenish Yellow
  [127, 24, 13, 255], // Strong Reddish Brown
  [147, 170, 0, 255], // Vivid Yellowish Green
  [89, 51, 21, 255], // Deep Yellowish Brown
  [241, 58, 19, 255], // Vivid Reddish Orange
  [35, 44, 22, 255], // Dark Olive Green
  [0, 161, 194, 255] // Vivid Blue
];
//-------------------------------------------------------------------------

// API reference:
// https://developers.google.com/mediapipe/api/solutions/js/tasks-vision.facelandmarker
//
async function createFaceLandmarker() {
  const filesetResolver = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
    //"./"
    //S3
  );
  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
    baseOptions: {
      modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
      //modelAssetPath: 'mp-face_landmarker.task',
      delegate: "GPU"
    },
    outputFaceBlendshapes: true,
    outputFacialTransformationMatrixes : true,
    runningMode,
    numFaces: 2
  });
  demosSection.classList.remove("invisible");

  console.log("createFaceLandmarker done");
}

createFaceLandmarker();

//-------------------------------------------------

const createImageSegmenter = async () => {
  const filesetResolver = await FilesetResolver.forVisionTasks("./");

  imageSegmenter = await ImageSegmenter.createFromOptions(filesetResolver, {
    baseOptions: {
      //modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/deeplab_v3/float32/1/deeplab_v3.tflite",
      modelAssetPath: "mp-deeplab_v3.tflite",
      delegate: "GPU"
    },
    runningMode: runningMode,
    outputCategoryMask: true,
    outputConfidenceMasks: false
  });
  labels = imageSegmenter.getLabels();
  demosSection.classList.remove("invisible");

  console.log("createImageSegmenter done");
};

//createImageSegmenter();

//-------------------------------------------------

const createPoseLandmarker = async () => {
  const filesetResolver = await FilesetResolver.forVisionTasks("./");
  poseLandmarker = await PoseLandmarker.createFromOptions(filesetResolver, {
    baseOptions: {
      //modelAssetPath: `https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task`,
      modelAssetPath: 'mp-pose_landmarker_lite.task',
      delegate: "GPU"
    },
    runningMode: runningMode,
    numPoses: 2
  });
  demosSection.classList.remove("invisible");

  console.log("createPoseLandmarker done");
};

//createPoseLandmarker();

//-------------------------------------------------

const createObjectDetector = async () => {
  const vision = await FilesetResolver.forVisionTasks("./");
  objectDetector = await ObjectDetector.createFromOptions(vision, {
    baseOptions: {
      //modelAssetPath: `https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/float16/1/efficientdet_lite0.tflite`,
      modelAssetPath: 'mp-efficientdet_lite0.tflite',
      delegate: "GPU"
    },
    scoreThreshold: 0.5,
    runningMode: runningMode
  });
  demosSection.classList.remove("invisible");

  console.log("createObjectDetector done");
};

//createObjectDetector();

//-------------------------------------------------

const createGestureRecognizer = async () => {
  const vision = await FilesetResolver.forVisionTasks("./");
  gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
    baseOptions: {
      //modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
      modelAssetPath: "mp-gesture_recognizer.task",
      delegate: "GPU"
    },
    numHands: 2,
    runningMode: runningMode
  });
  demosSection.classList.remove("invisible");

  console.log("createGestureRecognizer done");
};

//createGestureRecognizer();

//-------------------------------------------------

//const video         = document.getElementById("video");// as HTMLVideoElement;
const canvasElement = document.getElementById("output_canvas");// as HTMLCanvasElement;
const canvasCtx     = canvasElement ? canvasElement.getContext("2d") : null;

let list = new Array();

// Check if webcam access is supported.
function hasGetUserMedia() {
  return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}

// If webcam supported, add event listener to button for when user
// wants to activate it.
if (!hasGetUserMedia())
  console.warn("getUserMedia() is not supported by your browser");

enableWebcamButton = document.getElementById("startButton");// as HTMLButtonElement;
//enableWebcamButton.addEventListener("click", startWebCam);
enableWebcamButton.addEventListener("click", startAnalysis);

function startAnalysis(event)
{
  //startWebCam(event); // <-- use Webcam

  const startButton = document.getElementById('startButton');

  if(!webcamRunning)
  {
    webcamRunning = true; 
    startButton.textContent = 'STOP'; // Change text to STOP

    video.play(); 
    video.setAttribute("controls", "controls");

    doAnalysis(); // <-- use video
  }
  else if(webcamRunning)
  {
    webcamRunning = false; 
    startButton.textContent = 'START'; // Change text back to START

    video.pause();
    video.setAttribute("controls", "controls");

    let s = JSON.stringify(list, null, 2);

    //console.log("Face transform matrices:");
    //console.log(s);
    //console.log(list);

    const link = document.createElement("a");
    const file = new Blob([s], { type: 'text/plain' });

    link.href = URL.createObjectURL(file);
    link.download = "face-analysis.json";
    //link.click();
    URL.revokeObjectURL(link.href);
  }
}

// Enable the live webcam view and start detection.
function startWebCam(event)
{
  // if (!faceLandmarker) {
  //   console.log("Wait! faceLandmarker not loaded yet.");
  //   return;
  // }

  //console.log("startWebCam: webcamRunning=" + webcamRunning);

  if (webcamRunning === true) {
    webcamRunning = false;
    //enableWebcamButton.innerText = "ENABLE PREDICTIONS";
  } else {
    webcamRunning = true;
    //enableWebcamButton.innerText = "DISABLE PREDICTIONS";
  }

  // getUsermedia parameters.
  const constraints = {
    video: true
  };

  // Activate the webcam stream.
  try
  {
  navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
    video.srcObject = stream;

    // When webcam has started, start the analysis+render loop
    //video.addEventListener("loadeddata", doAnalysis);

    video.play(); 
  });
  }
  catch(e)
  {
    console.log("getUserMedia error: " + e);
  }
}

// Used by imageSegmenter.segmentForVideo(video, startTimeMs, callbackForVideo);
function callbackForVideo(result)//: ImageSegmenterResult)
{
  let imageData = canvasCtx.getImageData(
    0,
    0,
    video.videoWidth,
    video.videoHeight
  ).data;
  const mask//: Number[] 
              = result.categoryMask.getAsFloat32Array();
  let j = 0;
  for (let i = 0; i < mask.length; ++i) {
    const maskVal = Math.round(mask[i] * 255.0);
    const legendColor = legendColors[maskVal % legendColors.length];
    imageData[j    ] = (legendColor[0] + imageData[j    ]) / 2;
    imageData[j + 1] = (legendColor[1] + imageData[j + 1]) / 2;
    imageData[j + 2] = (legendColor[2] + imageData[j + 2]) / 2;
    imageData[j + 3] = (legendColor[3] + imageData[j + 3]) / 2;
    j += 4;
  }
  const uint8Array = new Uint8ClampedArray(imageData.buffer);
  const dataNew = new ImageData(
    uint8Array,
    video.videoWidth,
    video.videoHeight
  );
  canvasCtx.putImageData(dataNew, 0, 0);
}

// For Object Detector
// Keep a reference of all the child elements we create
// so we can remove them easilly on each render.
var children = [];
const liveView = document.getElementById("liveView");

function displayVideoDetections(result) {
  // Remove any highlighting from previous frame.
  for (let child of children) {
    liveView.removeChild(child);
  }
  children.splice(0);
  // Iterate through predictions and draw them to the live view
  for (let detection of result.detections) {
    const p = document.createElement("p");
    p.innerText =
      detection.categories[0].categoryName +
      " - with " +
      Math.round(parseFloat(detection.categories[0].score) * 100) +
      "% confidence.";
    p.style =
      "left: " +
      (video.offsetWidth -
        detection.boundingBox.width -
        detection.boundingBox.originX) +
      "px;" +
      "top: " +
      detection.boundingBox.originY +
      "px; " +
      "width: " +
      (detection.boundingBox.width - 10) +
      "px;";

    const highlighter = document.createElement("div");
    highlighter.setAttribute("class", "highlighter");
    highlighter.style =
      "left: " +
      (video.offsetWidth -
        detection.boundingBox.width -
        detection.boundingBox.originX) +
      "px;" +
      "top: " +
      detection.boundingBox.originY +
      "px;" +
      "width: " +
      (detection.boundingBox.width - 10) +
      "px;" +
      "height: " +
      detection.boundingBox.height +
      "px;";

    liveView.appendChild(highlighter);
    liveView.appendChild(p);

    // Store drawn objects in memory so they are queued to delete at next call.
    children.push(highlighter);
    children.push(p);
  }
}

const gestureOutput = document.getElementById("gesture_output");

let lastVideoTime = -1;
let results = undefined, resultsGR = undefined;
const drawingUtils = new DrawingUtils(canvasCtx);

/**
 * This will be merged with user supplied options.
 */
//const DEFAULT_OPTIONS: 
// let DrawingOptions = {
//   color: 'white',
//   lineWidth: 4,
//   radius: 6
// };

/** Merges the user's options with the default options. */
// function addDefaultOptions(style?: DrawingOptions): DrawingOptions
// {
//   style = style || {};
//   return {
//     ...DEFAULT_OPTIONS,
//     ...{fillColor: style.color},
//     ...style,
//   };
// }
function addDefaultOptions(style)
{
  const DEFAULT_OPTIONS = {
    color: 'white',
    lineWidth: 4,
    radius: 6
  };

  style = style || {};
  return {
    ...DEFAULT_OPTIONS,
    ...{ fillColor: style.color },
    ...style,
  };
}

/**
 * Resolve the value from `value`. Invokes `value` with `data` if it is a
 * function.
 */
//function resolve<O, I>(value: O|Callback<I, O>, data: I): O {
function resolve(value, data) {
  return value instanceof Function ? value(data) : value;
}

/**
 * Draws lines between landmarks (given a connection graph).
 *
 * This method can only be used when `DrawingUtils` is initialized with a
 * `CanvasRenderingContext2D`.
 *
 * @export
 * @param landmarks The landmarks to draw.
 * @param connections The connections array that contains the start and the
 *     end indices for the connections to draw.
 * @param style The style to visualize the landmarks.
 */
function drawConnectors(
    landmarks,//?: NormalizedLandmark[], 
    connections, //?: Connection[],
    style)//?: DrawingOptions): void
{
  if (!landmarks || !connections) {
    return;
  }

  const ctx = canvasCtx;//this.getCanvasRenderingContext();
  const options = addDefaultOptions(style);
  ctx.save();
  const canvas = ctx.canvas;
  let index = 0;

  for (const connection of connections)
  {
    ctx.beginPath();

    const from = landmarks[connection.start];
    const to   = landmarks[connection.end];
    
    if (from && to)
    {
      ctx.strokeStyle = resolve(options.color, {index, from, to});
      ctx.lineWidth = resolve(options.lineWidth, {index, from, to});
      ctx.moveTo(from.x * canvas.width, from.y * canvas.height);
      ctx.lineTo(to.x   * canvas.width, to.y   * canvas.height);
    }
    ++index;
    ctx.stroke();
  }

  ctx.restore();
}

function getRotation(matrix)
{
  let rotX, rotY, rotZ;
  
  // Handle gimbal lock cases first
  if (Math.abs(matrix[8]) >= 0.99999) { // m31 near ±1
      // Gimbal lock around X axis
      rotX = 0; // assume default angle for rotX
      rotY = Math.PI / 2 * Math.sign(matrix[8]); // 90° or -90°
      rotZ = Math.atan2(matrix[1], matrix[5]);
  } else {
      rotX = Math.atan2(-matrix[9], matrix[10]); // -m32, m33
      rotY = Math.asin(matrix[8]);               // m31
      rotZ = Math.atan2(-matrix[4], matrix[0]);  // -m21, m11
  }

  // Convert to degrees
  return {
      x: rotX * (180 / Math.PI),
      y: rotY * (180 / Math.PI),
      z: rotZ * (180 / Math.PI)
  };
}

function calculateFaceSize(landmarks)
{
  const leftIris  = landmarks[FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS[0].start];
  const rightIris = landmarks[FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS[0].start];

  const dx = rightIris.x - leftIris.x;
  const dy = rightIris.y - leftIris.y;
  const dz = rightIris.z - leftIris.z;

  // Euclidean distance
  const distance = Math.sqrt(dx * dx + dy * dy + dz * dz);

  return distance;
}

function updateFaceSizeDisplay(ms, faceSize)
{
  const faceSizeDisplay = document.getElementById('faceSizeDisplay');
  faceSizeDisplay.textContent = `Detect: ${ms.toFixed(2)}ms, ${(1000/ms).toFixed(0)}fps. Face size: ${faceSize.toFixed(4)}`;

    //console.log("detect: %fms, %ffps", ms.toFixed(3), (1000/ms).toFixed(0));

}

var faceDetected = false;

//
// Uses RAF: window.requestAnimationFrame(doAnalysis);
//
async function doAnalysis()
{
  const ratio = video.videoHeight / video.videoWidth;
  video.style.width  = videoWidth + "px";
  video.style.height = videoWidth * ratio + "px";
  
  if(canvasElement)
  {
  canvasElement.style.width  = videoWidth + "px";
  canvasElement.style.height = videoWidth * ratio + "px";
  canvasElement.width  = video.videoWidth;
  canvasElement.height = video.videoHeight;
  }

  if (runningMode === "IMAGE") {
    runningMode = "VIDEO";
    await faceLandmarker.setOptions({ runningMode: runningMode });
  }

  let startTimeMs = performance.now();
  if (lastVideoTime !== video.currentTime) {
    lastVideoTime = video.currentTime;

    // Calls to the Face Landmarker detect() and detectForVideo() methods run 
    // synchronously and block the user interface thread. If you detect faces 
    // in video frames from a device's camera, each detection blocks the main 
    // thread. You can prevent this by implementing web workers to run the 
    // detect() and detectForVideo() methods on another thread.
    //
    // (alias) type ImageSource = HTMLCanvasElement | OffscreenCanvas | ImageBitmap | 
    //                            ImageData | HTMLImageElement | HTMLVideoElement | 
    //                            VideoFrame
    // import ImageSource
    // Valid types of image sources which we can run our GraphRunner over.
    //
    // @deprecated — Use TexImageSource instead.
    //
    // face_landmarker.ts:
    // detectForVideo(videoFrame: ImageSource...)
    // processVideoData
    // vision_task_runner.ts:
    // private process(ImageSource...)
    // graphRunner.addGpuBufferAsImageToStream
    if(faceLandmarker)
      results = faceLandmarker.detectForVideo(video, startTimeMs);

    if(imageSegmenter)
      imageSegmenter.segmentForVideo(video, startTimeMs, callbackForVideo);

    if(poseLandmarker)
    poseLandmarker.detectForVideo(video, startTimeMs, (result) => {
      canvasCtx.save();
      //canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      for (const landmark of result.landmarks)
      {
        // drawingUtils.drawLandmarks(landmark, {
        //   radius: (data) => DrawingUtils.lerp(data.from!.z, -0.15, 0.1, 5, 1)
        // });
        drawingUtils.drawConnectors(landmark, PoseLandmarker.POSE_CONNECTIONS);
      }
      canvasCtx.restore();
    });

    if(objectDetector)
    {
      const detections = objectDetector.detectForVideo(video, startTimeMs);
      displayVideoDetections(detections);
    }

    if(gestureRecognizer)
    {
      resultsGR = gestureRecognizer.recognizeForVideo(video, startTimeMs);
    }
  }

  // M1 Max
  // 1920x1080: 1926074362_gettyvideo_FULL_HD.mp4
  // 2 faces: detect: 28.425ms, 35fps
  let ms = (performance.now()-startTimeMs);
  //console.log("detect: %fms, %ffps", ms.toFixed(3), (1000/ms).toFixed(0));

  if (resultsGR && resultsGR.landmarks) {
    for (const landmarks of resultsGR.landmarks) {
      drawingUtils.drawConnectors(
        landmarks,
        GestureRecognizer.HAND_CONNECTIONS,
        {
          color: "#00FF00",
          lineWidth: 5
        }
      );
      drawingUtils.drawLandmarks(landmarks, {
        color: "#FF0000",
        lineWidth: 2
      });
    }
  }

  // {faceLandmarks: Array(2), 
  //  faceBlendshapes: Array(2), 
  //  facialTransformationMatrixes: Array(2)}
  //console.log(results);

  var x,y,z;

  if(results && results.facialTransformationMatrixes[0])
  {
    if(!faceDetected)
    {
      showBD(true);
      faceDetected = true;
    }

    list.push(results.facialTransformationMatrixes[0]);


    // FaceLandmarker uses the matrix to transform the face landmarks 
    // from a canonical face model to the detected face, so users can 
    // apply effects on the detected landmarks.
    let mat = results.facialTransformationMatrixes[0].data;
    let vtx = results.faceLandmarks[0];
    let v0 = results.faceLandmarks[0][FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS[2].start];
    let v1 = results.faceLandmarks[0][FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS[0].start];

    // facialTransformationMatrixes[0].data:
    // +0.99, -0.08, +0.02, +0.00, 
    // +0.08, +0.98, -0.14, +0.00,
    // -0.01, +0.15, +0.98, +0.00, 
    // -0.73, +8.31, -47.53,+1.00

    // Between 2 iris's
    x = (v0.x+v1.x)/2; y = (v0.y+v1.y)/2; z = (v0.z+v1.z)/2;
    //let x = mat[12], y = mat[13], z = mat[14];
    //let x = vtx[0].x, y = vtx[0].y, z = vtx[0].z;


    let rot = getRotation(mat);

    // Extract scale from the matrix
    let scaleX = Math.sqrt(mat[0] * mat[0] + mat[1] * mat[1] + mat[2] * mat[2]).toFixed(3);
    let scaleY = Math.sqrt(mat[4] * mat[4] + mat[5] * mat[5] + mat[6] * mat[6]).toFixed(3);
    let scaleZ = Math.sqrt(mat[8] * mat[8] + mat[9] * mat[9] + mat[10] * mat[10]).toFixed(3);
    let scale = {scaleX, scaleY, scaleZ};

    // Always 1, why?
    //console.log("Scale: ", scaleX.toFixed(3), scaleY.toFixed(3), scaleZ.toFixed(3));

    // Calculate face size
    const faceSize = calculateFaceSize(results.faceLandmarks[0]);
    updateFaceSizeDisplay(ms, faceSize); // Update the face size display

    // This is size of the video
    const canvas = canvasCtx.canvas;
    const w = canvas.width, h = canvas.height;

    // This is size defined in the <video> tag
    const vidElement = document.getElementById('video');
    const vw = vidElement.width;//clientWidth;
    const vh = vidElement.height;//clientHeight;
    const ar = vh / vw; // BBC news video seems to suggest AR adjustment is needed

    updateBDPosition(x * vw,
                     y * vh,// / ar,
                     -rot.z,
                    faceSize / 0.07);

    // Notice the canvas is the actual size of video and video element is size of div
    // canvas:  1280 720 , video:  1000 563
    // canvas is video size (eg 1920x1080) but div size is different
    //console.log("canvas: ", w, h, 
    // ", video: ", vidElement.clientWidth, vidElement.clientHeight);

    //console.log("pos: ", x.toFixed(3), ",", y.toFixed(3), ", rotation: ", rot,
    //            "scale: ", scale);

    //console.log(mat);
    //console.log(mat[3], ", ", mat[4+3]);

    for (const m of results.facialTransformationMatrixes)
    {
      // {rows: 4, columns: 4, data: Array(16)}
      //console.log(m);
    }
  }
  else
  {
    if(faceDetected)
    {
      showBD(false);
      faceDetected = false;
    }
  }

  if(results && results.faceLandmarks)
  {
    for (const landmarks of results.faceLandmarks)
    {

      // {x: 0.4073820412158966, y: 0.49272677302360535, z: -0.012607785873115063}
      //console.log(landmarks[0]);

      // const drawingUtils = new DrawingUtils(canvasCtx);

      /** Landmarks for face tesselation. Index into 468 vertices. */
      // export const FACE_LANDMARKS_TESSELATION = convertToConnections(
      // [127, 34], [34, 139], [139, 127], [11, 0], [0, 37], [37, 11], [232, 231],
      // [231, 120], [120, 232], [72, 37], [37, 39], [39, 72], [128, 121], [121, 47],

      // Draw outlines of specific features
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_TESSELATION, { color: "#C0C0C070", lineWidth: 1 });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE, { color: "#FF3030" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW, { color: "#FF3030" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_EYE, { color: "#30FF30" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW, { color: "#30FF30" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_FACE_OVAL, { color: "#E0E0E0" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LIPS, { color: "#E0E0E0" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS, { color: "#D03030" });
      drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS, { color: "#30D030" });

      // Draw small circles at FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS
      const leftIrisLandmarks = FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS;
      const rightIrisLandmarks = FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS;
      const cw = canvasElement.width, ch = canvasElement.height;

      var ex, ey;

      //for (const index of leftIrisLandmarks)
      {
        const index = leftIrisLandmarks[2];

        const landmark = landmarks[index.start];
        canvasCtx.beginPath();
        canvasCtx.arc(landmark.x * cw, landmark.y * ch,
                      3, 0, 3 * Math.PI);
        canvasCtx.fillStyle = "#FFFF30";
        canvasCtx.fill();

        ex = landmark.x * cw;
        ey = landmark.y * ch;

        //break;
      }

      //for (const index of rightIrisLandmarks)
      {
        const index = rightIrisLandmarks[0];

        const landmark = landmarks[index.start];
        canvasCtx.beginPath();
        canvasCtx.arc(landmark.x * cw, landmark.y * ch,
          3, 0, 3 * Math.PI);
        canvasCtx.fillStyle = "#FFFF30";
        canvasCtx.fill();

        ex = (ex + landmark.x * cw) / 2;
        ey = (ey + landmark.y * ch) / 2;

        //break;
      }
  
      canvasCtx.beginPath();
      //canvasCtx.arc(x * cw, y * ch, 4, 0, 4 * Math.PI);
      canvasCtx.arc(ex, ey, 4, 0, 4 * Math.PI);
      canvasCtx.fillStyle = "#FF40FF";
      canvasCtx.fill();
    }
  }

  // Draw confidence as horizontal bar charts
  //drawBlendShapes(videoBlendShapes, results.faceBlendshapes);

  if(webcamRunning === true)
    window.requestAnimationFrame(doAnalysis);
}

function drawBlendShapes(el, blendShapes) {
  if (!blendShapes.length) {
    return;
  }

  //console.log(blendShapes[0]);
  
  let htmlMaker = "";
  blendShapes[0].categories.map((shape) => {
    htmlMaker += `
      <li class="blend-shapes-item">
        <span class="blend-shapes-label">${
          shape.displayName || shape.categoryName
        }</span>
        <span class="blend-shapes-value" style="width: calc(${
          +shape.score * 100
        }% - 120px)">${(+shape.score).toFixed(4)}</span>
      </li>
    `;
  });

  el.innerHTML = htmlMaker;
}

// Add event listener to the webcam start button
document.getElementById('webcamStartButton').addEventListener('click', startWebCam);

</script>
</body>
</html>